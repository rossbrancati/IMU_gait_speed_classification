{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "186e8f81",
   "metadata": {},
   "source": [
    "# Pre-Processing Data for training and testing models \n",
    "\n",
    "This notebook walks through the pre-processing steps to take the data from the open source reporitory and structure it in matrices that are ready to train and test machine and deep learning models.<br>\n",
    "\n",
    "You will have to change file paths and create folders either locally or online if you plan to use this script to pre-process the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf9340a",
   "metadata": {},
   "source": [
    "### Objective of Restructuring Data\n",
    "\n",
    "We want to structure our data so that we have each signal from a sensor in its own matrix where each row of the matrix is a stride and each column of the matrix is a timepoint of the gait cycle. \n",
    "\n",
    "Since there are 4 sensors and 3 (angular velocity) signals per sensor, there will be 12 total matrices. \n",
    "\n",
    "I resampled to 100 datapoints to convert strides to 0-100% of the gait cycle. \n",
    "\n",
    "The following function takes one data file (and its indices for heel strikes), crops each signal from right heel strike to right heel strike (RHS to RHS), resamples the signal to 1000 data points, and stores it in a matrix. This function is repeatedly called in a lower cell to automatically restructure the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8770854c",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1) Restructuring Data<br>\n",
    "2) Standardizign Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d140c642",
   "metadata": {},
   "source": [
    "# Part 1: Restructuring Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da158faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd656bdb",
   "metadata": {},
   "source": [
    "### Right Shank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fe254c",
   "metadata": {},
   "source": [
    "Function that crops the right leg signals from right heel strike (RHS) to the next consecutive RHS, time normalizes the signal from 0-100% of the gait cycle, and concatenates each stride into a matrix where was row is a stride and each column is a timepoint of the gait cycle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1bea2170",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for cropping right leg signals to RHS-RHS and concatenating them into a single matrix for each variable\n",
    "def right_leg_data_wrangler(data, indices, variable, subject_ID, speed, trial):\n",
    "    data_matrix = []\n",
    "    \n",
    "    #loop over each row of the indices data, pulling the two heel strike indexes for each stride\n",
    "    for i in range(len(indices)-1):\n",
    "    \n",
    "        #heel strike indices\n",
    "        first_RHS = indices['RHS'].values[i]\n",
    "        second_RHS = indices['RHS'].values[i+1]\n",
    "\n",
    "        #get vector between current heel strikes\n",
    "        stride = data[variable].loc[first_RHS:second_RHS]\n",
    "\n",
    "        #resample stride to 1000 data points\n",
    "        resampled_stride = resample(stride,101)\n",
    "        \n",
    "        #insert subject_id and speed into dataframe\n",
    "        resampled_stride = np.append(resampled_stride, trial)\n",
    "        resampled_stride = np.append(resampled_stride, speed)\n",
    "        resampled_stride = np.append(resampled_stride, subject_ID)\n",
    "\n",
    "        #append data matrix\n",
    "        data_matrix.append(resampled_stride)\n",
    "        \n",
    "    #convert to dataframe\n",
    "    data_frame = pd.DataFrame(data_matrix)\n",
    "        \n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a805e35e",
   "metadata": {},
   "source": [
    "### Set Directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567d1a4f",
   "metadata": {},
   "source": [
    "Set the working directory to the folder containing the data from the open source repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "159b6f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = sorted(os.listdir(os.getcwd()+'/data'))\n",
    "#remove .DS_Store file if it exists\n",
    "if '.DS_Store' in data_folder:\n",
    "    data_folder.remove('.DS_Store')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10be9a7c",
   "metadata": {},
   "source": [
    "### Restructure data and save \n",
    "\n",
    "The next cell loops over the directory, loads each file, and calls the right_leg_data_wrangler function to restructure the data. Data files are then exported to the \"variable_matrices\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f4568ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#right limb variables of interest\n",
    "GYRx_taR_all = pd.DataFrame()\n",
    "GYRy_taR_all = pd.DataFrame()\n",
    "GYRz_taR_all = pd.DataFrame()\n",
    "GYRx_tbR_all = pd.DataFrame()\n",
    "GYRy_tbR_all = pd.DataFrame()\n",
    "GYRz_tbR_all = pd.DataFrame()\n",
    "\n",
    "\n",
    "#loop over all subjects\n",
    "for subject_idx,subject_name in enumerate(data_folder):\n",
    "    #assign current subject directory\n",
    "    subject_folder = sorted(os.listdir(os.getcwd()+'/data/'+subject_name))\n",
    "    #loop over files in directory\n",
    "    for file_idx, file_name in enumerate(subject_folder):\n",
    "        #ignore any files with 'ev' or 'up'\n",
    "        if 'ev' not in file_name:\n",
    "            if 'up' not in file_name:\n",
    "                #assign current data file and load\n",
    "                data_path = os.path.join(os.getcwd()+'/data/'+subject_name+'/'+file_name)\n",
    "                data = pd.read_csv(data_path,sep='\\t')\n",
    "                #assign current indices file and load\n",
    "                indices_path = os.path.join(os.getcwd()+'/data/'+subject_name+'/'+file_name[:-4]+'ev.txt')\n",
    "                indices = pd.read_csv(indices_path,sep='\\t')\n",
    "\n",
    "                #using the function from above, generate matrices where each row is a stride and each column is a timepoint of the gait cycle\n",
    "                #easiest to do this without a loop\n",
    "                #6 right leg metrics = 6 pandas dataframes\n",
    "                \n",
    "                #GYRx_taR\n",
    "                GYRx_taR = right_leg_data_wrangler(data, indices, 'GYRx_taR', file_name[0:3], file_name[3], file_name[4])\n",
    "                GYRx_taR_all = GYRx_taR_all.append(GYRx_taR)\n",
    "                #GYRy_taR\n",
    "                GYRy_taR = right_leg_data_wrangler(data, indices, 'GYRy_taR', file_name[0:3], file_name[3], file_name[4])\n",
    "                GYRy_taR_all = GYRy_taR_all.append(GYRy_taR)\n",
    "                #GYRz_taR\n",
    "                GYRz_taR = right_leg_data_wrangler(data, indices, 'GYRz_taR', file_name[0:3], file_name[3], file_name[4])\n",
    "                GYRz_taR_all = GYRz_taR_all.append(GYRz_taR)\n",
    "\n",
    "                #GYRx_tbR\n",
    "                GYRx_tbR = right_leg_data_wrangler(data, indices, 'GYRx_tbR', file_name[0:3], file_name[3], file_name[4])\n",
    "                GYRx_tbR_all = GYRx_tbR_all.append(GYRx_tbR)\n",
    "                #GYRy_tbR\n",
    "                GYRy_tbR = right_leg_data_wrangler(data, indices, 'GYRy_tbR', file_name[0:3], file_name[3], file_name[4])\n",
    "                GYRy_tbR_all = GYRy_tbR_all.append(GYRy_tbR)\n",
    "                #GYRz_tbR\n",
    "                GYRz_tbR = right_leg_data_wrangler(data, indices, 'GYRz_tbR', file_name[0:3], file_name[3], file_name[4])\n",
    "                GYRz_tbR_all = GYRz_tbR_all.append(GYRz_tbR)\n",
    "                \n",
    "\n",
    "                \n",
    "#move last three columns to beginning of pandas dataframes and rename\n",
    "GYRx_taR_all_1 = GYRx_taR_all.pop(101)\n",
    "GYRx_taR_all.insert(0, 'trial', GYRx_taR_all_1)\n",
    "GYRx_taR_all_2 = GYRx_taR_all.pop(102)\n",
    "GYRx_taR_all.insert(0, 'speed', GYRx_taR_all_2)\n",
    "GYRx_taR_all_3 = GYRx_taR_all.pop(103)\n",
    "GYRx_taR_all.insert(0, 'subject_ID', GYRx_taR_all_3)\n",
    "\n",
    "GYRy_taR_all_1 = GYRy_taR_all.pop(101)\n",
    "GYRy_taR_all.insert(0, 'trial', GYRy_taR_all_1)\n",
    "GYRy_taR_all_2 = GYRy_taR_all.pop(102)\n",
    "GYRy_taR_all.insert(0, 'speed', GYRy_taR_all_2)\n",
    "GYRy_taR_all_3 = GYRy_taR_all.pop(103)\n",
    "GYRy_taR_all.insert(0, 'subject_ID', GYRy_taR_all_3)\n",
    "\n",
    "GYRz_taR_all_1 = GYRz_taR_all.pop(101)\n",
    "GYRz_taR_all.insert(0, 'trial', GYRz_taR_all_1)\n",
    "GYRz_taR_all_2 = GYRz_taR_all.pop(102)\n",
    "GYRz_taR_all.insert(0, 'speed', GYRz_taR_all_2)\n",
    "GYRz_taR_all_3 = GYRz_taR_all.pop(103)\n",
    "GYRz_taR_all.insert(0, 'subject_ID', GYRz_taR_all_3)\n",
    "\n",
    "GYRx_tbR_all_1 = GYRx_tbR_all.pop(101)\n",
    "GYRx_tbR_all.insert(0, 'trial', GYRx_tbR_all_1)\n",
    "GYRx_tbR_all_2 = GYRx_tbR_all.pop(102)\n",
    "GYRx_tbR_all.insert(0, 'speed', GYRx_tbR_all_2)\n",
    "GYRx_tbR_all_3 = GYRx_tbR_all.pop(103)\n",
    "GYRx_tbR_all.insert(0, 'subject_ID', GYRx_tbR_all_3)\n",
    "\n",
    "GYRy_tbR_all_1 = GYRy_tbR_all.pop(101)\n",
    "GYRy_tbR_all.insert(0, 'trial', GYRy_tbR_all_1)\n",
    "GYRy_tbR_all_2 = GYRy_tbR_all.pop(102)\n",
    "GYRy_tbR_all.insert(0, 'speed', GYRy_tbR_all_2)\n",
    "GYRy_tbR_all_3 = GYRy_tbR_all.pop(103)\n",
    "GYRy_tbR_all.insert(0, 'subject_ID', GYRy_tbR_all_3)\n",
    "\n",
    "GYRz_tbR_all_1 = GYRz_tbR_all.pop(101)\n",
    "GYRz_tbR_all.insert(0, 'trial', GYRz_tbR_all_1)\n",
    "GYRz_tbR_all_2 = GYRz_tbR_all.pop(102)\n",
    "GYRz_tbR_all.insert(0, 'speed', GYRz_tbR_all_2)\n",
    "GYRz_tbR_all_3 = GYRz_tbR_all.pop(103)\n",
    "GYRz_tbR_all.insert(0, 'subject_ID', GYRz_tbR_all_3)\n",
    "\n",
    "\n",
    "#export matrices to .csv files\n",
    "pd.DataFrame.to_csv(GYRx_taR_all, os.getcwd()+'/pre_processing/variable_matrices/'+'GYRx_taR.csv', sep=',')\n",
    "pd.DataFrame.to_csv(GYRy_taR_all, os.getcwd()+'/pre_processing/variable_matrices/'+'GYRy_taR.csv', sep=',')\n",
    "pd.DataFrame.to_csv(GYRz_taR_all, os.getcwd()+'/pre_processing/variable_matrices/'+'GYRz_taR.csv', sep=',')\n",
    "\n",
    "pd.DataFrame.to_csv(GYRx_tbR_all, os.getcwd()+'/pre_processing/variable_matrices/'+'GYRx_tbR.csv', sep=',')\n",
    "pd.DataFrame.to_csv(GYRy_tbR_all, os.getcwd()+'/pre_processing/variable_matrices/'+'GYRy_tbR.csv', sep=',')\n",
    "pd.DataFrame.to_csv(GYRz_tbR_all, os.getcwd()+'/pre_processing/variable_matrices/'+'GYRz_tbR.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee3a957",
   "metadata": {},
   "source": [
    "I ended up running the above cell 3 separate times because my laptop didn't have enough computing power to handle all of the files at once. I divided the 22 subjects into 3 groups, ran the previous 2 cells separately for each group, then used the following cell to concatenate all of the files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e8c685b1",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12102\n",
      "12102\n",
      "12102\n",
      "12102\n",
      "12102\n",
      "12102\n"
     ]
    }
   ],
   "source": [
    "#create list of variables\n",
    "variable_list = ['GYRx_taR', 'GYRy_taR', 'GYRz_taR', 'GYRx_tbR', 'GYRy_tbR', 'GYRz_tbR']\n",
    "#loop over variables in list\n",
    "for i in variable_list:\n",
    "    #load files\n",
    "    file_1 = pd.read_csv(os.getcwd()+'/pre_processing/variable_matrices_1/'+i+'.csv', index_col=0)\n",
    "    file_2 = pd.read_csv(os.getcwd()+'/pre_processing/variable_matrices_2/'+i+'.csv', index_col=0)\n",
    "    file_3 = pd.read_csv(os.getcwd()+'/pre_processing/variable_matrices_3/'+i+'.csv', index_col=0)\n",
    "    #concatenate files\n",
    "    master_file = pd.concat([file_1, file_2, file_3])\n",
    "    #save file\n",
    "    pd.DataFrame.to_csv(master_file, os.getcwd()+'/strides/variable_matrices_strides/'+i+'.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8593fd",
   "metadata": {},
   "source": [
    "Lastly, lets horizontally concatenate the files from all 6 variables so we are left with one matrix with 12102 rows and 600 variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0b2cc5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of variables\n",
    "variable_list = ['GYRx_taR', 'GYRy_taR', 'GYRz_taR', 'GYRx_tbR', 'GYRy_tbR', 'GYRz_tbR']\n",
    "R_variables_all = pd.DataFrame()\n",
    "#loop over variables in list\n",
    "for i in variable_list:\n",
    "    #load files\n",
    "    current_file = pd.read_csv(os.getcwd()+'/strides/variable_matrices_strides/'+i+'.csv', index_col=0)\n",
    "    #horizontally concatenate files\n",
    "    R_variables_all = pd.concat([R_variables_all, current_file], axis=1)\n",
    "\n",
    "    \n",
    "#pop off repeated columns (subject_ID, trial, speed)\n",
    "subject_ID = R_variables_all.pop('subject_ID').iloc[:,1]\n",
    "speed = R_variables_all.pop('speed').iloc[:,1]\n",
    "trial = R_variables_all.pop('trial').iloc[:,1]\n",
    "#and reinsert one instance at beginning of dataframe\n",
    "R_variables_all.insert(0, 'trial', trial)\n",
    "R_variables_all.insert(0, 'speed', speed)\n",
    "R_variables_all.insert(0, 'subject_ID', subject_ID)\n",
    "\n",
    "#save file    \n",
    "pd.DataFrame.to_csv(R_variables_all, os.getcwd()+'/strides/ML_data/R_variables_all.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe6de92",
   "metadata": {},
   "source": [
    "Now we have one matrix with 12102 observations and 606 variables, where each observation is a stride from a subject at a specific walking speed and each variable is a datapoint from a percentage of the gait cycle from one of the IMU's angular velocity signals. This is for the right limb only, so next we'll repeat for the left shank."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7731da",
   "metadata": {},
   "source": [
    "### Left Shank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a0036e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for cropping left leg signals to RHS-RHS and concatenating them into a single matrix for each variable\n",
    "def left_leg_data_wrangler(data, indices, variable, subject_ID, speed, trial):\n",
    "    data_matrix = []\n",
    "    \n",
    "    #loop over each row of the indices data, pulling the two heel strike indexes for each stride\n",
    "    for i in range(len(indices)-1):\n",
    "    \n",
    "        #heel strike indices\n",
    "        first_LHS = indices['LHS'].values[i]\n",
    "        second_LHS = indices['LHS'].values[i+1]\n",
    "\n",
    "        #get vector between current heel strikes\n",
    "        stride = data[variable].loc[first_LHS:second_LHS]\n",
    "\n",
    "        #resample stride to 1000 data points\n",
    "        resampled_stride = resample(stride,101)\n",
    "        \n",
    "        #insert subject_id and speed into dataframe\n",
    "        resampled_stride = np.append(resampled_stride, trial)\n",
    "        resampled_stride = np.append(resampled_stride, speed)\n",
    "        resampled_stride = np.append(resampled_stride, subject_ID)\n",
    "\n",
    "        #append data matrix\n",
    "        data_matrix.append(resampled_stride)\n",
    "        \n",
    "    #convert to dataframe\n",
    "    data_frame = pd.DataFrame(data_matrix)\n",
    "        \n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f2c3f6",
   "metadata": {},
   "source": [
    "### Set Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c63fbad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = sorted(os.listdir(os.getcwd()+'/data'))\n",
    "#remove .DS_Store file if it exists\n",
    "if '.DS_Store' in data_folder:\n",
    "    data_folder.remove('.DS_Store')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757585e2",
   "metadata": {},
   "source": [
    "### Restructure data and save\n",
    "\n",
    "The next cell loops over the directory, loads each file, and calls the left_leg_data_wrangler function to restructure the data. Data files are then exported to the \"variable_matrices\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9e7a5c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#left limb variables of interest\n",
    "GYRx_taL_all = pd.DataFrame()\n",
    "GYRy_taL_all = pd.DataFrame()\n",
    "GYRz_taL_all = pd.DataFrame()\n",
    "GYRx_tbL_all = pd.DataFrame()\n",
    "GYRy_tbL_all = pd.DataFrame()\n",
    "GYRz_tbL_all = pd.DataFrame()\n",
    "\n",
    "\n",
    "#loop over all subjects\n",
    "for subject_idx,subject_name in enumerate(data_folder):\n",
    "    #assign current subject directory\n",
    "    subject_folder = sorted(os.listdir(os.getcwd()+'/data/'+subject_name))\n",
    "    #loop over files in directory\n",
    "    for file_idx, file_name in enumerate(subject_folder):\n",
    "        #ignore any files with 'ev' or 'up'\n",
    "        if 'ev' not in file_name:\n",
    "            if 'up' not in file_name:\n",
    "                #assign current data file and load\n",
    "                data_path = os.path.join(os.getcwd()+'/data/'+subject_name+'/'+file_name)\n",
    "                data = pd.read_csv(data_path,sep='\\t')\n",
    "                #assign current indices file and load\n",
    "                indices_path = os.path.join(os.getcwd()+'/data/'+subject_name+'/'+file_name[:-4]+'ev.txt')\n",
    "                indices = pd.read_csv(indices_path,sep='\\t')\n",
    "\n",
    "                #using the function from above, generate matrices where each row is a stride and each column is a timepoint of the gait cycle\n",
    "                #easiest to do this without a loop\n",
    "                #6 left leg metrics = 6 pandas dataframes\n",
    "                \n",
    "                #GYRx_taL\n",
    "                GYRx_taL = left_leg_data_wrangler(data, indices, 'GYRx_taL', file_name[0:3], file_name[3], file_name[4])\n",
    "                GYRx_taL_all = GYRx_taL_all.append(GYRx_taL)\n",
    "                #GYRy_taL\n",
    "                GYRy_taL = left_leg_data_wrangler(data, indices, 'GYRy_taL', file_name[0:3], file_name[3], file_name[4])\n",
    "                GYRy_taL_all = GYRy_taL_all.append(GYRy_taL)\n",
    "                #GYRz_taL\n",
    "                GYRz_taL = left_leg_data_wrangler(data, indices, 'GYRz_taL', file_name[0:3], file_name[3], file_name[4])\n",
    "                GYRz_taL_all = GYRz_taL_all.append(GYRz_taL)\n",
    "\n",
    "                #GYRx_tbL\n",
    "                GYRx_tbL = left_leg_data_wrangler(data, indices, 'GYRx_tbL', file_name[0:3], file_name[3], file_name[4])\n",
    "                GYRx_tbL_all = GYRx_tbL_all.append(GYRx_tbL)\n",
    "                #GYRy_tbL\n",
    "                GYRy_tbL = left_leg_data_wrangler(data, indices, 'GYRy_tbL', file_name[0:3], file_name[3], file_name[4])\n",
    "                GYRy_tbL_all = GYRy_tbL_all.append(GYRy_tbL)\n",
    "                #GYRz_tbL\n",
    "                GYRz_tbL = left_leg_data_wrangler(data, indices, 'GYRz_tbL', file_name[0:3], file_name[3], file_name[4])\n",
    "                GYRz_tbL_all = GYRz_tbL_all.append(GYRz_tbL)\n",
    "                \n",
    "\n",
    "                \n",
    "#move last three columns to beginning of pandas dataframes and rename\n",
    "GYRx_taL_all_1 = GYRx_taL_all.pop(101)\n",
    "GYRx_taL_all.insert(0, 'trial', GYRx_taL_all_1)\n",
    "GYRx_taL_all_2 = GYRx_taL_all.pop(102)\n",
    "GYRx_taL_all.insert(0, 'speed', GYRx_taL_all_2)\n",
    "GYRx_taL_all_3 = GYRx_taL_all.pop(103)\n",
    "GYRx_taL_all.insert(0, 'subject_ID', GYRx_taL_all_3)\n",
    "\n",
    "GYRy_taL_all_1 = GYRy_taL_all.pop(101)\n",
    "GYRy_taL_all.insert(0, 'trial', GYRy_taL_all_1)\n",
    "GYRy_taL_all_2 = GYRy_taL_all.pop(102)\n",
    "GYRy_taL_all.insert(0, 'speed', GYRy_taL_all_2)\n",
    "GYRy_taL_all_3 = GYRy_taL_all.pop(103)\n",
    "GYRy_taL_all.insert(0, 'subject_ID', GYRy_taL_all_3)\n",
    "\n",
    "GYRz_taL_all_1 = GYRz_taL_all.pop(101)\n",
    "GYRz_taL_all.insert(0, 'trial', GYRz_taL_all_1)\n",
    "GYRz_taL_all_2 = GYRz_taL_all.pop(102)\n",
    "GYRz_taL_all.insert(0, 'speed', GYRz_taL_all_2)\n",
    "GYRz_taL_all_3 = GYRz_taL_all.pop(103)\n",
    "GYRz_taL_all.insert(0, 'subject_ID', GYRz_taL_all_3)\n",
    "\n",
    "GYRx_tbL_all_1 = GYRx_tbL_all.pop(101)\n",
    "GYRx_tbL_all.insert(0, 'trial', GYRx_tbL_all_1)\n",
    "GYRx_tbL_all_2 = GYRx_tbL_all.pop(102)\n",
    "GYRx_tbL_all.insert(0, 'speed', GYRx_tbL_all_2)\n",
    "GYRx_tbL_all_3 = GYRx_tbL_all.pop(103)\n",
    "GYRx_tbL_all.insert(0, 'subject_ID', GYRx_tbL_all_3)\n",
    "\n",
    "GYRy_tbL_all_1 = GYRy_tbL_all.pop(101)\n",
    "GYRy_tbL_all.insert(0, 'trial', GYRy_tbL_all_1)\n",
    "GYRy_tbL_all_2 = GYRy_tbL_all.pop(102)\n",
    "GYRy_tbL_all.insert(0, 'speed', GYRy_tbL_all_2)\n",
    "GYRy_tbL_all_3 = GYRy_tbL_all.pop(103)\n",
    "GYRy_tbL_all.insert(0, 'subject_ID', GYRy_tbL_all_3)\n",
    "\n",
    "GYRz_tbL_all_1 = GYRz_tbL_all.pop(101)\n",
    "GYRz_tbL_all.insert(0, 'trial', GYRz_tbL_all_1)\n",
    "GYRz_tbL_all_2 = GYRz_tbL_all.pop(102)\n",
    "GYRz_tbL_all.insert(0, 'speed', GYRz_tbL_all_2)\n",
    "GYRz_tbL_all_3 = GYRz_tbL_all.pop(103)\n",
    "GYRz_tbL_all.insert(0, 'subject_ID', GYRz_tbL_all_3)\n",
    "\n",
    "\n",
    "#export matrices to .csv files\n",
    "pd.DataFrame.to_csv(GYRx_taL_all, os.getcwd()+'/pre_processing/variable_matrices/'+'GYRx_taL.csv', sep=',')\n",
    "pd.DataFrame.to_csv(GYRy_taL_all, os.getcwd()+'/pre_processing/variable_matrices/'+'GYRy_taL.csv', sep=',')\n",
    "pd.DataFrame.to_csv(GYRz_taL_all, os.getcwd()+'/pre_processing/variable_matrices/'+'GYRz_taL.csv', sep=',')\n",
    "\n",
    "pd.DataFrame.to_csv(GYRx_tbL_all, os.getcwd()+'/pre_processing/variable_matrices/'+'GYRx_tbL.csv', sep=',')\n",
    "pd.DataFrame.to_csv(GYRy_tbL_all, os.getcwd()+'/pre_processing/variable_matrices/'+'GYRy_tbL.csv', sep=',')\n",
    "pd.DataFrame.to_csv(GYRz_tbL_all, os.getcwd()+'/pre_processing/variable_matrices/'+'GYRz_tbL.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d52725",
   "metadata": {},
   "source": [
    "I ended up running the above cell 3 separate times because my laptop didn't have enough computing power to handle all of the files at once. I divided the 22 subjects into 3 groups, ran the previous 2 cells separately for each group, then used the following cell to concatenate all of the files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1ed460d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of variables\n",
    "variable_list = ['GYRx_taL', 'GYRy_taL', 'GYRz_taL', 'GYRx_tbL', 'GYRy_tbL', 'GYRz_tbL']\n",
    "#loop over variables in list\n",
    "for i in variable_list:\n",
    "    #load files\n",
    "    file_1 = pd.read_csv(os.getcwd()+'/pre_processing/variable_matrices_1/'+i+'.csv', index_col=0)\n",
    "    file_2 = pd.read_csv(os.getcwd()+'/pre_processing/variable_matrices_2/'+i+'.csv', index_col=0)\n",
    "    file_3 = pd.read_csv(os.getcwd()+'/pre_processing/variable_matrices_3/'+i+'.csv', index_col=0)\n",
    "    #concatenate files\n",
    "    master_file = pd.concat([file_1, file_2, file_3])\n",
    "    #save file\n",
    "    pd.DataFrame.to_csv(master_file, os.getcwd()+'/strides/variable_matrices_strides/'+i+'.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c1b04d",
   "metadata": {},
   "source": [
    "Lastly, lets horizontally concatenate the files from all 6 variables so we are left with one matrix with 12102 rows and 600 variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "983803bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of variables\n",
    "variable_list = ['GYRx_taL', 'GYRy_taL', 'GYRz_taL', 'GYRx_tbL', 'GYRy_tbL', 'GYRz_tbL']\n",
    "L_variables_all = pd.DataFrame()\n",
    "#loop over variables in list\n",
    "for i in variable_list:\n",
    "    #load files\n",
    "    current_file = pd.read_csv(os.getcwd()+'/strides/variable_matrices_strides/'+i+'.csv', index_col=0)\n",
    "    #horizontally concatenate files\n",
    "    L_variables_all = pd.concat([L_variables_all, current_file], axis=1)\n",
    "\n",
    "    \n",
    "#pop off repeated columns (subject_ID, trial, speed)\n",
    "subject_ID = L_variables_all.pop('subject_ID').iloc[:,1]\n",
    "speed = L_variables_all.pop('speed').iloc[:,1]\n",
    "trial = L_variables_all.pop('trial').iloc[:,1]\n",
    "#and reinsert one instance at beginning of dataframe\n",
    "L_variables_all.insert(0, 'trial', trial)\n",
    "L_variables_all.insert(0, 'speed', speed)\n",
    "L_variables_all.insert(0, 'subject_ID', subject_ID)\n",
    "\n",
    "#save file    \n",
    "pd.DataFrame.to_csv(L_variables_all, os.getcwd()+'/strides/ML_data/L_variables_all.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9725ae65",
   "metadata": {},
   "source": [
    "## Part 2: Standardizing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c50292",
   "metadata": {},
   "source": [
    "It is important to standardize data before feeding it into machine learning models. Variables with larger ranges may hold more weight when training and validating ML models. For example, we could expect that motions in the sagittal plane (i.e., when the shank moves forward to back) would produce larger angular velocity values compared to the frontal plane (i.e., when the shank moves from left to right). Standardizing time-series data is a little bit different than traditional standardization in ML modeling. To do this, I typically use a z-transoformation by subtracting the mean and dividing by the standard deviation. This keeps the same overall shape of the waveform, which is really what we care about. However, it removes the discrepencies between signals in the ranges of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "9c81020e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop over each folder\n",
    "for file_idx, file_name in enumerate(variable_matrices_folder):\n",
    "    #load file\n",
    "    file_path = os.path.join(os.getcwd()+'/strides/variable_matrices_strides/'+file_name)\n",
    "    variable_matrix = pd.read_csv(file_path,sep=',', index_col=0)\n",
    "    #drop the subject_ID, speed, and trial columns\n",
    "    variable_matrix_values = variable_matrix.drop(['subject_ID', 'speed', 'trial'], axis=1)\n",
    "    \n",
    "    #z-transformation\n",
    "    variable_matrix_minus_mean = variable_matrix_values.sub(variable_matrix_values.mean(axis=1), axis=0)\n",
    "    variable_matrix_std = variable_matrix_minus_mean.divide(variable_matrix_minus_mean.std(axis=1), axis=0)\n",
    "    \n",
    "    #add the columns back to the dataframe\n",
    "    variable_matrix_std.insert(0, 'subject_ID', variable_matrix['subject_ID'])\n",
    "    variable_matrix_std.insert(1, 'speed', variable_matrix['speed'])\n",
    "    variable_matrix_std.insert(2, 'trial', variable_matrix['trial'])\n",
    "    \n",
    "    #export standardized dataframes\n",
    "    pd.DataFrame.to_csv(variable_matrix_std, os.getcwd()+'/strides/variable_matrices_strides_standard/'+file_name[:-4]+'_std.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ecc32c",
   "metadata": {},
   "source": [
    "Next, concatenate all data into one file where each row is an observation and each column is a timepoint for each of the variables (total of 600 columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "b134fe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of variables\n",
    "variable_list = ['GYRx_taR_std', 'GYRy_taR_std', 'GYRz_taR_std', 'GYRx_tbR_std', 'GYRy_tbR_std', 'GYRz_tbR_std']\n",
    "R_variables_all = pd.DataFrame()\n",
    "#loop over variables in list\n",
    "for i in variable_list:\n",
    "    #load files\n",
    "    current_file = pd.read_csv(os.getcwd()+'/strides/variable_matrices_strides_standard/'+i+'.csv', index_col=0)\n",
    "    #horizontally concatenate files\n",
    "    R_variables_all = pd.concat([R_variables_all, current_file], axis=1)\n",
    "\n",
    "    \n",
    "#pop off repeated columns (subject_ID, trial, speed)\n",
    "subject_ID = R_variables_all.pop('subject_ID').iloc[:,1]\n",
    "speed = R_variables_all.pop('speed').iloc[:,1]\n",
    "trial = R_variables_all.pop('trial').iloc[:,1]\n",
    "#and reinsert one instance at beginning of dataframe\n",
    "R_variables_all.insert(0, 'trial', trial)\n",
    "R_variables_all.insert(0, 'speed', speed)\n",
    "R_variables_all.insert(0, 'subject_ID', subject_ID)\n",
    "\n",
    "#save file    \n",
    "pd.DataFrame.to_csv(R_variables_all, os.getcwd()+'/strides/ML_data/R_variables_all_std.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "e0c5bd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of variables\n",
    "variable_list = ['GYRx_taL_std', 'GYRy_taL_std', 'GYRz_taL_std', 'GYRx_tbL_std', 'GYRy_tbL_std', 'GYRz_tbL_std']\n",
    "L_variables_all = pd.DataFrame()\n",
    "#loop over variables in list\n",
    "for i in variable_list:\n",
    "    #load files\n",
    "    current_file = pd.read_csv(os.getcwd()+'/strides/variable_matrices_strides_standard/'+i+'.csv', index_col=0)\n",
    "    #horizontally concatenate files\n",
    "    L_variables_all = pd.concat([L_variables_all, current_file], axis=1)\n",
    "\n",
    "    \n",
    "#pop off repeated columns (subject_ID, trial, speed)\n",
    "subject_ID = L_variables_all.pop('subject_ID').iloc[:,1]\n",
    "speed = L_variables_all.pop('speed').iloc[:,1]\n",
    "trial = L_variables_all.pop('trial').iloc[:,1]\n",
    "#and reinsert one instance at beginning of dataframe\n",
    "L_variables_all.insert(0, 'trial', trial)\n",
    "L_variables_all.insert(0, 'speed', speed)\n",
    "L_variables_all.insert(0, 'subject_ID', subject_ID)\n",
    "\n",
    "#save file    \n",
    "pd.DataFrame.to_csv(L_variables_all, os.getcwd()+'/strides/ML_data/L_variables_all_std.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687ddd4f",
   "metadata": {},
   "source": [
    "Now we can think about performing some machine learning modeling. One approach would be to feed the standardized IMU signals directly into a model, however that might be a longer training and validation process because of the large number of variables. Another approach would be to extract some features from the data such as statistical metrics (mean, median, range, max, min, etc.) and/or principal components. Lets try the principal components first and see how well that performs. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
